{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\", FutureWarning)\n",
    "\n",
    "import xclone_config\n",
    "project_config = xclone_config\n",
    "os.chdir(project_config.ROOT)\n",
    "\n",
    "from collections import defaultdict, OrderedDict\n",
    "import multiprocessing as mp\n",
    "import sys\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from data_types import ase\n",
    "import plotlib\n",
    "import toolkit\n",
    "import util\n",
    "import test_phasing\n",
    "from workspace.workspace_manager import WorkspaceManager\n",
    "from data_types import ase\n",
    "\n",
    "workspace = WorkspaceManager(\n",
    "    task_name=\"preprocessing\",\n",
    "    experiment_info={\"sample\" : \"N5CC3E\", \n",
    "                     \"modality\" : \"scATAC\"},\n",
    "    verbose=True\n",
    ")\n",
    "workspace.load_workspace()\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'snp_counts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-565482ede0d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m data = {\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata_type\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmp_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"snp_counts\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"phasing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"blocks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snp\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snp_counts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-565482ede0d9>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m data = {\n\u001b[1;32m      2\u001b[0m     \u001b[0mdata_type\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpickle_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkspace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtmp_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"snp_counts\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"phasing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"blocks\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m }\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snp\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"snp_counts\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'snp_counts'"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    data_type : util.pickle_load(workspace.tmp_data[data_type])\n",
    "    for data_type in [\"snp_counts\", \"phasing\", \"blocks\"]\n",
    "}\n",
    "data[\"snp\"] = data[\"snp_counts\"]\\\n",
    "                [[\"CHROM\", \"POS\"]]\\\n",
    "                .to_dense()\\\n",
    "                .astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chrom_to_blocks = {\n",
    "    str(chrom) : data[\"blocks\"][data[\"blocks\"][\"CHROM\"] == chrom]\n",
    "    for chrom in data[\"blocks\"][\"CHROM\"].unique()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Here block coverage for each phased SNP is computed\n",
    "# TODO: rewrite this using \"bedtools intersect\".\n",
    "# This part doesn't scale well.\n",
    "\n",
    "def intersect_snp_with_blocks(snp_tuple):\n",
    "    chrom, pos = snp_tuple\n",
    "    # 1-based to 0-based \n",
    "    pos -= 1 # because CellSNP is 1-based, but .bed files are 0-based\n",
    "    blocks_on_chrom = chrom_to_blocks.get(str(chrom), None)\n",
    "    if blocks_on_chrom is None:\n",
    "        return \"\"\n",
    "    mask = ((blocks_on_chrom.START <= pos) \n",
    "            & (pos < blocks_on_chrom.END))\n",
    "    return '@'.join(blocks_on_chrom[mask].BLOCK_ID)\n",
    "\n",
    "# result = [intersect_snp_with_blocks(snp_tuple) \n",
    "#           for snp_tuple in tqdm_notebook(data[\"snp\"][data[\"snp\"][\"CHROM\"] == 10].values, \n",
    "#                                          \"SNP processing\")]\n",
    "pool = mp.Pool(16)\n",
    "result = pool.map(intersect_snp_with_blocks, \n",
    "                  tqdm_notebook(data[\"snp\"].values, \n",
    "                                \"SNP processing\"))\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Here the raw results computed in parallel are parsed \"\"\"\n",
    "snp_to_blocks, block_to_snps = {}, OrderedDict()\n",
    "\n",
    "# This way we ensure proper block ordering\n",
    "for block_id in data[\"blocks\"].BLOCK_ID:\n",
    "    block_to_snps[block_id] = []\n",
    "\n",
    "for i, row in tqdm_notebook(enumerate(data[\"snp\"].values), \n",
    "                            \"mapping SNPs to blocks\"):\n",
    "    chrom, pos = row\n",
    "    snp = f\"{chrom},{pos}\"\n",
    "    snp_to_blocks[snp] = result[i].split(\"@\") if result[i] else []\n",
    "    for block in snp_to_blocks[snp]:\n",
    "        block_to_snps[block].append(snp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "sns.set(font_scale=1.5)\n",
    "plt.title(f\"{workspace.experiment_info['modality']}, \"\n",
    "          \"number of blocks covering one SNP\")\n",
    "plt.xlabel(\"number of blocks\")\n",
    "plt.ylabel(\"snp count\")\n",
    "plt.hist([len(block_list) for block_list in snp_to_blocks.values()]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.title(f\"{workspace.experiment_info['modality']}\"\n",
    "          \", number of SNPs covered by block\")\n",
    "plt.xlabel(\"number of snps\")\n",
    "plt.ylabel(\"block count\")\n",
    "plt.hist([len(snp_list) for snp_list \n",
    "          in block_to_snps.values()]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.pickle_dump(\n",
    "    snp_to_blocks, \n",
    "    f\"{workspace.tmp_dir}/snp_to_blocks.pkl\"\n",
    ")\n",
    "workspace.add_entry(\"snp_to_blocks\", \"snp_to_blocks.pkl\")\n",
    "util.pickle_dump(\n",
    "    block_to_snps, \n",
    "    f\"{workspace.tmp_dir}/block_to_snps.pkl\"\n",
    ")\n",
    "workspace.add_entry(\"block_to_snps\", \"block_to_snps.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace.status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_to_idx = {snp : i for i, snp in \n",
    "              tqdm_notebook(enumerate(toolkit.extract_snps(data[\"snp_counts\"])), \n",
    "                            \"mapping snps to their index numbers \"\n",
    "                            \"by position in the blocks\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "block_to_snp_ids = {block : np.array([snp_to_idx[snp] for snp in snp_list]) \n",
    "                   for block, snp_list \n",
    "                   in tqdm_notebook(block_to_snps.items(), \n",
    "                                    desc=\"mapping block to SNP ids\"\n",
    "                                    \" (for faster row selection)\")}\n",
    "\n",
    "import numba\n",
    "# @numba.jit(nopython=True)\n",
    "def get_block_counts(ad, dp, snp_ids):\n",
    "    if snp_ids.size > 0:\n",
    "#         mask = np.zeros(ad.size).astype(np.bool)\n",
    "#         mask[snp_ids] = True\n",
    "#         ad_sample, dp_sample = ad[s], dp[mask]\n",
    "        return np.nansum(ad[snp_ids]), np.nansum(dp[snp_ids])\n",
    "    return np.nan, np.nan\n",
    "\n",
    "\n",
    "def extract_block_counts(barcode):\n",
    "    block_to_ad, block_to_dp = [], []\n",
    "    # I need to convert these columns to dense format\n",
    "    # because I need only a subset of rows on each iteration \n",
    "    ad = np.array(data[\"snp_counts\"][f\"{barcode}_ad\"])\n",
    "    dp = np.array(data[\"snp_counts\"][f\"{barcode}_dp\"])\n",
    "    \n",
    "    # block_to_snps is an OrderedDict, so we can guarantee\n",
    "    # that all the blocks are processed in a correct order\n",
    "    for snp_ids in block_to_snp_ids.values():\n",
    "        if len(snp_ids) > 0:\n",
    "            dp_sum = np.nansum(dp[snp_ids])\n",
    "            block_to_dp.append(np.nan if dp_sum == 0 else dp_sum)\n",
    "            block_to_ad.append(np.nan if dp_sum == 0 \n",
    "                               else np.nansum(ad[snp_ids]))\n",
    "    \n",
    "    return pd.DataFrame({f\"{barcode}_ad\" : block_to_ad, \n",
    "                         f\"{barcode}_dp\" : block_to_dp})\n",
    "\n",
    "\n",
    "pool = mp.Pool(16)\n",
    "result_list = pool.map(\n",
    "    extract_block_counts, \n",
    "    tqdm_notebook(toolkit.extract_barcodes(data[\"snp_counts\"]), \n",
    "                  desc=\"cell_barcode processing\")\n",
    ")\n",
    "pool.close()\n",
    "pool.join()\n",
    "# result_list = [extract_block_counts(barcode) for barcode in \n",
    "#                tqdm_notebook(toolkit.extract_barcodes(data[\"snp_counts\"]), \n",
    "#                              desc=\"cell_barcode processing\")]\n",
    "data[\"block_counts\"] = pd.concat(result_list, axis=1)\n",
    "data[\"block_counts\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[\"block_counts\"].insert(\n",
    "    0, \"BLOCK_ID\", \n",
    "    [block for block, snp_list \n",
    "     in block_to_snps.items() \n",
    "     if len(snp_list) > 0]\n",
    ")\n",
    "data[\"block_counts\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"clustering\"] = util.pickle_load(f\"{workspace.tmp_dir}/clustering.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_barcodes = data[\"clustering\"][\"BARCODE\"]\n",
    "# raw_barcodes = toolkit.extract_barcodes(data[\"block_counts\"])\n",
    "# noisy_barcodes = np.setdiff1d(raw_barcodes, clean_barcodes)\n",
    "# data[\"block_counts\"].drop(\n",
    "#     columns=[f\"{barcode}_ad\" for barcode in noisy_barcodes],\n",
    "#     inplace=True\n",
    "# )\n",
    "# data[\"block_counts\"].drop(\n",
    "#     columns=[f\"{barcode}_dp\" for barcode in noisy_barcodes],\n",
    "#     inplace=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_blocks = data[\"blocks\"][\n",
    "    data[\"blocks\"].BLOCK_ID.isin(data[\"block_counts\"].BLOCK_ID)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "data[\"block_counts\"] = data[\"block_counts\"].merge(\n",
    "    remaining_blocks,\n",
    "    on=\"BLOCK_ID\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"block_counts\"] = data[\"block_counts\"].to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "util.pickle_dump(\n",
    "    data[\"block_counts\"],\n",
    "    os.path.join(\n",
    "        workspace.tmp_dir,\n",
    "        \"block_counts_T1.pkl\"\n",
    "    )\n",
    ")\n",
    "workspace.add_entry(\"block_counts_T1\", \"block_counts_T1.pkl\")\n",
    "workspace.verify()\n",
    "workspace.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ase\"] = ase.compute_ase(\n",
    "    data[\"block_counts\"], \n",
    "    toolkit.extract_barcodes(data[\"block_counts\"])\n",
    ").to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"ase\"].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.pickle_dump(\n",
    "    data[\"ase\"],\n",
    "    os.path.join(\n",
    "        workspace.tmp_dir,\n",
    "        \"ase_T1.pkl\"\n",
    "    )\n",
    ")\n",
    "workspace.add_entry(\"ase_T1\", \"ase_T1.pkl\")\n",
    "workspace.verify()\n",
    "workspace.push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"block_counts\"][[\"BLOCK_ID\", \"CHROM\", \"START\", \"END\"]].to_csv(\n",
    "    os.path.join(\n",
    "        workspace.dir,\n",
    "        \"annotation_T1.csv\"\n",
    "    ), index=False\n",
    ")\n",
    "with open(os.path.join(workspace.dir, \"barcodes_T1.txt\"), \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(toolkit.extract_barcodes(data[\"block_counts\"])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-xclone] *",
   "language": "python",
   "name": "conda-env-.conda-xclone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
